#!/usr/bin/env node
/**
 * CIS File Scanner - Main Entry Point
 * NASãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦AWS S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
 */

import * as dotenv from 'dotenv';
import { program } from 'commander';
import * as cron from 'node-cron';
import { FileSystemAdapterFactory } from '@/adapters';
import { FileScanner, DatabaseManager, ProgressTracker, S3Uploader, SQSPublisher, SQSConsumer, PowerShellRunner } from '@/services';
import { AWSConfig, NASConfig, ScannerConfig, SyncResult } from '@/types';
import { createLogger } from '@/utils/logger';
import * as path from 'path';

// ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
dotenv.config();

const logger = createLogger('Main');

/**
 * ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã‚’èª­ã¿è¾¼ã¿
 */
function loadConfig(): {
  aws: AWSConfig;
  nas: NASConfig;
  scanner: ScannerConfig;
} {
  return {
    aws: {
      region: process.env.AWS_REGION || 'ap-northeast-1',
      accessKeyId: process.env.AWS_ACCESS_KEY_ID,
      secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
      s3: {
        bucket: process.env.S3_BUCKET_NAME || 'cis-filesearch-landing',
        uploadConcurrency: parseInt(process.env.S3_UPLOAD_CONCURRENCY || '10'),
        multipartThreshold: parseInt(process.env.S3_MULTIPART_THRESHOLD_MB || '100'),
        chunkSize: parseInt(process.env.S3_MULTIPART_CHUNK_SIZE_MB || '10')
      },
      sqs: {
        queueUrl: process.env.SQS_QUEUE_URL || '',
        dlqUrl: process.env.SQS_DLQ_URL
      }
    },
    nas: {
      protocol: (process.env.NAS_PROTOCOL as any) || 'auto',
      mountPath: process.env.NAS_MOUNT_PATH || '/mnt/nas',
      host: process.env.NAS_HOST,
      username: process.env.NAS_USERNAME,
      password: process.env.NAS_PASSWORD,
      domain: process.env.NAS_DOMAIN
    },
    scanner: {
      nasPath: process.env.NAS_MOUNT_PATH || '/mnt/nas',
      s3Bucket: process.env.S3_BUCKET_NAME || 'cis-filesearch-landing',
      sqsQueueUrl: process.env.SQS_QUEUE_URL || '',
      batchSize: parseInt(process.env.SCAN_BATCH_SIZE || '1000'),
      parallelism: parseInt(process.env.SCAN_PARALLELISM || '20'),
      excludePatterns: (process.env.SCAN_EXCLUDE_PATTERNS || '').split(',').filter(p => p),
      maxFileSize: process.env.SCAN_MAX_FILE_SIZE_MB
        ? parseInt(process.env.SCAN_MAX_FILE_SIZE_MB) * 1024 * 1024
        : undefined,
      dryRun: process.env.DRY_RUN === 'true'
    }
  };
}

/**
 * ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã‚’å®Ÿè¡Œ
 */
async function runFullScan(options: any = {}) {
  logger.info('========================================');
  logger.info('Starting Full Scan');
  logger.info('========================================');

  const config = loadConfig();

  // ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã®ç¢ºèª
  if (options.dryRun || config.scanner.dryRun) {
    logger.warn('DRY RUN MODE ENABLED - No actual uploads will be performed');
  }

  try {
    // ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
    const adapter = FileSystemAdapterFactory.createFromEnv();
    const database = new DatabaseManager({
      dbPath: process.env.DB_PATH
    });
    const progressTracker = new ProgressTracker({
      enableLogging: true
    });

    // ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã«æ¥ç¶š
    await adapter.connect();

    // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–
    await database.initialize();

    // ã‚¹ã‚­ãƒ£ãƒŠãƒ¼ã‚’ä½œæˆ
    const scanner = new FileScanner({
      adapter,
      database,
      excludePatterns: config.scanner.excludePatterns,
      maxFileSize: config.scanner.maxFileSize,
      concurrency: config.scanner.parallelism,
      batchSize: config.scanner.batchSize,
      onProgress: (event) => {
        if (event.percentage % 10 === 0) {
          logger.info(`Scan progress: ${event.percentage}% (${event.current}/${event.total})`);
        }
      },
      dryRun: options.dryRun || config.scanner.dryRun
    });

    // S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
    const uploader = new S3Uploader({
      awsConfig: config.aws,
      adapter,
      database,
      progressTracker,
      dryRun: options.dryRun || config.scanner.dryRun
    });

    // SQSãƒ‘ãƒ–ãƒªãƒƒã‚·ãƒ£ãƒ¼ã‚’ä½œæˆ
    const publisher = new SQSPublisher({
      awsConfig: config.aws,
      dryRun: options.dryRun || config.scanner.dryRun
    });

    // ã‚¹ã‚­ãƒ£ãƒ³ã‚’å®Ÿè¡Œ
    logger.info(`Scanning directory: ${config.scanner.nasPath}`);
    const scanResult = await scanner.startScan(config.scanner.nasPath);

    logger.info('Scan completed:');
    logger.info(`  Total files: ${scanResult.totalFiles}`);
    logger.info(`  Total size: ${(scanResult.totalSize / 1024 / 1024 / 1024).toFixed(2)} GB`);
    logger.info(`  New files: ${scanResult.newFiles.length}`);
    logger.info(`  Modified files: ${scanResult.modifiedFiles.length}`);
    logger.info(`  Deleted files: ${scanResult.deletedFiles.length}`);
    logger.info(`  Errors: ${scanResult.errors.length}`);

    // æ–°è¦ãƒ»å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if (scanResult.newFiles.length > 0 || scanResult.modifiedFiles.length > 0) {
      logger.info('Starting file upload to S3...');

      const filesToUpload = [...scanResult.newFiles, ...scanResult.modifiedFiles];
      const uploadResults = await uploader.uploadBatch(filesToUpload);

      logger.info(`Upload completed: ${uploadResults.length} files uploaded`);

      // SQSã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
      if (!options.skipSqs && config.aws.sqs.queueUrl) {
        logger.info('Publishing messages to SQS...');

        for (const [index, result] of uploadResults.entries()) {
          const fileMetadata = filesToUpload[index];
          if (fileMetadata) {
            await publisher.publishFileUploaded(fileMetadata, result);
          }
        }

        logger.info('SQS messages published');
      }
    }

    // çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜
    await database.saveScanHistory({
      rootPath: config.scanner.nasPath,
      totalFiles: scanResult.totalFiles,
      totalSize: scanResult.totalSize,
      newFiles: scanResult.newFiles.length,
      modifiedFiles: scanResult.modifiedFiles.length,
      deletedFiles: scanResult.deletedFiles.length,
      errors: scanResult.errors.length,
      duration: scanResult.scanDuration,
      status: 'completed'
    });

    // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    await database.cleanup();
    await database.close();
    await adapter.disconnect();
    await uploader.cleanup();
    await publisher.cleanup();

    logger.info('Full scan completed successfully');

  } catch (error) {
    logger.error('Full scan failed:', error);
    process.exit(1);
  }
}

/**
 * å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³ã‚’å®Ÿè¡Œ
 */
async function runDifferentialScan(options: any = {}) {
  logger.info('========================================');
  logger.info('Starting Differential Scan');
  logger.info('========================================');

  const config = loadConfig();

  try {
    // ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
    const adapter = FileSystemAdapterFactory.createFromEnv();
    const database = new DatabaseManager();

    await adapter.connect();
    await database.initialize();

    // æœ€å¾Œã®ã‚¹ã‚­ãƒ£ãƒ³æ™‚åˆ»ã‚’å–å¾—
    const stats = await database.getStatistics();
    const lastScanTime = stats.lastScanTime;

    if (!lastScanTime) {
      logger.warn('No previous scan found. Running full scan instead.');
      return await runFullScan(options);
    }

    logger.info(`Last scan: ${lastScanTime.toISOString()}`);

    // ã‚¹ã‚­ãƒ£ãƒŠãƒ¼ã‚’ä½œæˆ
    const scanner = new FileScanner({
      adapter,
      database,
      excludePatterns: config.scanner.excludePatterns,
      maxFileSize: config.scanner.maxFileSize,
      concurrency: config.scanner.parallelism,
      dryRun: options.dryRun || config.scanner.dryRun
    });

    // å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³ã‚’å®Ÿè¡Œ
    const scanResult = await scanner.quickScan(config.scanner.nasPath, lastScanTime);

    logger.info(`Differential scan found ${scanResult.modifiedFiles.length} changed files`);

    // å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if (scanResult.modifiedFiles.length > 0) {
      const uploader = new S3Uploader({
        awsConfig: config.aws,
        adapter,
        database,
        dryRun: options.dryRun || config.scanner.dryRun
      });

      await uploader.uploadBatch(scanResult.modifiedFiles);
      await uploader.cleanup();
    }

    // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    await database.close();
    await adapter.disconnect();

    logger.info('Differential scan completed successfully');

  } catch (error) {
    logger.error('Differential scan failed:', error);
    process.exit(1);
  }
}

/**
 * ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã•ã‚ŒãŸã‚¹ã‚­ãƒ£ãƒ³ã‚’é–‹å§‹
 */
function startScheduledScan(cronExpression: string) {
  logger.info(`Starting scheduled scan with cron: ${cronExpression}`);

  const task = cron.schedule(cronExpression, async () => {
    logger.info('Scheduled scan triggered');
    await runDifferentialScan();
  });

  task.start();
  logger.info('Scheduled scan started. Press Ctrl+C to stop.');

  // ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  process.on('SIGINT', () => {
    logger.info('Stopping scheduled scan...');
    task.stop();
    process.exit(0);
  });
}

/**
 * çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
 */
async function showStatistics() {
  const database = new DatabaseManager();
  await database.initialize();

  const stats = await database.getStatistics();
  const history = await database.getScanHistory(5);

  console.log('\n========================================');
  console.log('File Scanner Statistics');
  console.log('========================================\n');

  console.log('Current Database:');
  console.log(`  Total files: ${stats.totalFiles}`);
  console.log(`  Total size: ${(stats.totalSize / 1024 / 1024 / 1024).toFixed(2)} GB`);
  console.log(`  Last scan: ${stats.lastScanTime?.toISOString() || 'Never'}`);

  console.log('\nFile Types (Top 10):');
  const fileTypes = Object.entries(stats.fileTypes)
    .sort((a, b) => b[1] - a[1])
    .slice(0, 10);

  for (const [ext, count] of fileTypes) {
    console.log(`  ${ext}: ${count} files`);
  }

  console.log('\nRecent Scan History:');
  for (const scan of history) {
    console.log(`  ${scan.scanTime.toISOString()}: ${scan.totalFiles} files, ${scan.status}`);
  }

  await database.close();
}

/**
 * SQSã‚­ãƒ¥ãƒ¼ã‚’è¨ºæ–­
 */
async function diagnoseSQS() {
  logger.info('========================================');
  logger.info('SQS Queue Diagnostics');
  logger.info('========================================');

  const config = loadConfig();

  if (!config.aws.sqs.queueUrl) {
    logger.error('SQS_QUEUE_URL is not set');
    process.exit(1);
  }

  try {
    const publisher = new SQSPublisher({
      awsConfig: config.aws,
      dryRun: false
    });

    // ã‚­ãƒ¥ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
    const metrics = await publisher.getQueueMetrics();

    console.log('\nğŸ“Š Queue Metrics:');
    console.log(`  Messages in Queue: ${metrics.approximateNumberOfMessages}`);
    console.log(`  Messages in Flight: ${metrics.approximateNumberOfMessagesNotVisible}`);
    console.log(`  Messages Delayed: ${metrics.approximateNumberOfMessagesDelayed}`);

    // å‡¦ç†é€Ÿåº¦ã®åˆ†æ
    const totalPendingMessages = metrics.approximateNumberOfMessages +
                         metrics.approximateNumberOfMessagesNotVisible;

    console.log('\nâš ï¸ Analysis:');
    console.log(`  Total pending (queue + in-flight): ${totalPendingMessages}`);

    if (metrics.approximateNumberOfMessages > 1000) {
      console.log(`  âš ï¸ High backlog detected: ${metrics.approximateNumberOfMessages} messages`);
      console.log('  Recommendation: Increase python-worker instances');
    } else if (metrics.approximateNumberOfMessages > 100) {
      console.log(`  âš ï¸ Moderate backlog: ${metrics.approximateNumberOfMessages} messages`);
      console.log('  Recommendation: Monitor worker processing speed');
    } else {
      console.log(`  âœ… Queue is healthy: ${metrics.approximateNumberOfMessages} messages`);
    }

    if (metrics.approximateNumberOfMessagesNotVisible > 500) {
      console.log(`  âš ï¸ High in-flight messages: ${metrics.approximateNumberOfMessagesNotVisible}`);
      console.log('  Possible issue: Workers not deleting messages after processing');
    }

    // DLQç¢ºèª
    if (config.aws.sqs.dlqUrl) {
      console.log('\nğŸ”´ Checking Dead Letter Queue...');
      const dlqPublisher = new SQSPublisher({
        awsConfig: {
          ...config.aws,
          sqs: {
            queueUrl: config.aws.sqs.dlqUrl,
            dlqUrl: undefined
          }
        },
        dryRun: false
      });

      const dlqMetrics = await dlqPublisher.getQueueMetrics();
      console.log(`  DLQ Messages: ${dlqMetrics.approximateNumberOfMessages}`);

      if (dlqMetrics.approximateNumberOfMessages > 0) {
        console.log('  âš ï¸ Failed messages detected in DLQ!');
        console.log('  Action required: Investigate and reprocess failed messages');
      } else {
        console.log('  âœ… No failed messages in DLQ');
      }

      await dlqPublisher.cleanup();
    }

    // çµ±è¨ˆæƒ…å ±
    const stats = publisher.getStatistics();
    console.log('\nğŸ“ˆ Publisher Statistics:');
    console.log(`  Published: ${stats.publishedCount}`);
    console.log(`  Failed: ${stats.failedCount}`);
    console.log(`  Queued: ${stats.queuedCount}`);

    // æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    console.log('\nğŸ’¡ Recommendations:');

    const backlog = metrics.approximateNumberOfMessages;
    const estimatedHours = backlog / (10 * 60); // 10 files/min

    if (backlog > 100) {
      console.log('  1. Scale up python-worker instances:');
      console.log(`     - Current backlog: ${backlog} messages`);
      console.log(`     - Estimated time to clear (1 worker): ${estimatedHours.toFixed(1)} hours`);
      console.log('     - Recommended: 4-8 worker instances');

      console.log('\n  2. Enable batch processing in python-worker:');
      console.log('     - Change sqs_max_messages from 1 to 10');
      console.log('     - Implement bulk indexing in OpenSearch');

      console.log('\n  3. Monitor CloudWatch Logs:');
      console.log('     - Check for processing errors');
      console.log('     - Verify OpenSearch connection');
    } else {
      console.log('  âœ… System is operating normally');
    }

    await publisher.cleanup();

  } catch (error) {
    logger.error('Diagnostics failed:', error);
    process.exit(1);
  }
}

/**
 * SQS Consumerãƒ¢ãƒ¼ãƒ‰ã§ã‚¹ã‚­ãƒ£ãƒ³ã‚’å®Ÿè¡Œ
 * ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã®NASåŒæœŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†
 *
 * å‡¦ç†ãƒ•ãƒ­ãƒ¼:
 * 1. SQSã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡
 * 2. PowerShellã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆnas-sync-improved.ps1ï¼‰ã‚’å®Ÿè¡Œ
 *    - NASã‹ã‚‰ incoming/ ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
 *    - DocuWorks Converter ã¨ DataSync Monitor ãŒå¾Œç¶šå‡¦ç†
 * 3. DynamoDBã«é€²æ—ã‚’æ›´æ–°
 */
async function startSQSConsumer() {
  logger.info('========================================');
  logger.info('Starting SQS Consumer Mode');
  logger.info('========================================');

  const config = loadConfig();

  // å¿…é ˆç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
  const syncQueueUrl = process.env.SYNC_SQS_QUEUE_URL;
  const dynamoTableName = process.env.SYNC_DYNAMODB_TABLE;
  const nasSyncScriptPath = process.env.NAS_SYNC_SCRIPT_PATH || 'C:\\CIS-FileSearch\\scripts\\nas-sync-improved.ps1';
  const syncMode = process.env.SYNC_MODE || 'auto'; // 'powershell' | 'nodejs' | 'auto'

  if (!syncQueueUrl) {
    logger.error('SYNC_SQS_QUEUE_URL environment variable is required');
    process.exit(1);
  }

  if (!dynamoTableName) {
    logger.error('SYNC_DYNAMODB_TABLE environment variable is required');
    process.exit(1);
  }

  logger.info(`Queue URL: ${syncQueueUrl}`);
  logger.info(`DynamoDB Table: ${dynamoTableName}`);
  logger.info(`Sync Mode: ${syncMode}`);

  // å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®æ±ºå®š
  const isWindows = PowerShellRunner.isWindowsEnvironment();
  const usePowerShell = syncMode === 'powershell' || (syncMode === 'auto' && isWindows);

  if (usePowerShell) {
    logger.info(`PowerShell Script: ${nasSyncScriptPath}`);
    logger.info('Mode: PowerShell (nas-sync-improved.ps1 + DocuWorks Converter + DataSync Monitor)');
  } else {
    logger.info('Mode: Node.js (built-in file scanner - DocuWorks processing not available)');
    logger.warn('WARNING: DocuWorks OCR processing is not available in Node.js mode');
  }

  // ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆ: è¨±å¯ã•ã‚ŒãŸNASã‚µãƒ¼ãƒãƒ¼å
  const allowedNasServers = new Set([
    'ts-server3', 'ts-server5', 'ts-server6', 'ts-server7'
  ]);

  // UUIDå½¢å¼ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
  const isValidUuid = (str: string): boolean => {
    return /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(str);
  };

  // ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œé–¢æ•°ã‚’å®šç¾©ï¼ˆPowerShellç‰ˆï¼‰
  const scanExecutorPowerShell = async (options: {
    syncId: string;
    nasServers: string[];
    fullSync: boolean;
    onProgress?: (current: number, total: number, currentNas: string, processedFiles: number) => void;
  }): Promise<SyncResult> => {
    const { syncId, nasServers, fullSync, onProgress } = options;

    // å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if (!isValidUuid(syncId)) {
      throw new Error(`Invalid syncId format: ${syncId}`);
    }

    for (const server of nasServers) {
      if (!allowedNasServers.has(server)) {
        throw new Error(`Invalid NAS server: ${server}. Allowed: ${Array.from(allowedNasServers).join(', ')}`);
      }
    }

    logger.info(`Executing PowerShell sync for ${syncId}`);
    logger.info(`  NAS Servers: ${nasServers.join(', ')}`);
    logger.info(`  Full Sync: ${fullSync}`);

    // PowerShellRunnerã‚’ä½œæˆ
    const runner = new PowerShellRunner({
      scriptPath: nasSyncScriptPath,
      workingDirectory: path.dirname(nasSyncScriptPath),
      timeout: 4 * 60 * 60 * 1000, // 4æ™‚é–“
      environment: {
        AWS_REGION: config.aws.region,
        AWS_PROFILE: process.env.AWS_PROFILE || 'cis-scanner'
      }
    });

    try {
      // é€²æ—æ›´æ–°ï¼ˆé–‹å§‹ï¼‰
      if (onProgress) {
        onProgress(0, nasServers.length, 'starting', 0);
      }

      // PowerShellã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
      const result = await runner.executeNasSync({
        fullSync,
        dryRun: config.scanner.dryRun,
        onProgress: (message) => {
          logger.debug(`PowerShell: ${message}`);
          // é€²æ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰NASåã¨å‡¦ç†æ•°ã‚’æŠ½å‡º
          const nasMatch = message.match(/\[([^\]]+)\]/);
          if (nasMatch?.[1] && onProgress) {
            const currentNas = nasMatch[1];
            const processedMatch = message.match(/(\d+)\s*\/\s*(\d+)/);
            if (processedMatch?.[1] && processedMatch?.[2]) {
              const current = parseInt(processedMatch[1], 10);
              const total = parseInt(processedMatch[2], 10);
              onProgress(current, total, currentNas, current);
            }
          }
        }
      });

      logger.info('PowerShell sync completed:');
      logger.info(`  New files: ${result.new_files}`);
      logger.info(`  Changed files: ${result.changed_files}`);
      logger.info(`  Deleted files: ${result.deleted_files}`);
      logger.info(`  Synced: ${result.synced}`);
      logger.info(`  Errors: ${result.errors}`);

      return {
        newFiles: result.new_files,
        changedFiles: result.changed_files,
        deletedFiles: result.deleted_files,
        syncedFiles: result.synced,
        errors: result.errors
      };

    } catch (error) {
      logger.error('PowerShell sync failed:', error);
      throw error;
    }
  };

  // ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œé–¢æ•°ã‚’å®šç¾©ï¼ˆNode.jsç‰ˆ - é–‹ç™º/ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
  const scanExecutorNodeJs = async (options: {
    syncId: string;
    nasServers: string[];
    fullSync: boolean;
    onProgress?: (current: number, total: number, currentNas: string, processedFiles: number) => void;
  }): Promise<SyncResult> => {
    const { syncId, nasServers, fullSync, onProgress } = options;

    logger.info(`Executing Node.js scan for sync ${syncId}`);
    logger.info(`  NAS Servers: ${nasServers.join(', ')}`);
    logger.info(`  Full Sync: ${fullSync}`);

    let totalNewFiles = 0;
    let totalChangedFiles = 0;
    let totalDeletedFiles = 0;
    let totalSyncedFiles = 0;
    let totalErrors = 0;

    // å„NASã‚µãƒ¼ãƒãƒ¼ã‚’ã‚¹ã‚­ãƒ£ãƒ³
    for (let i = 0; i < nasServers.length; i++) {
      const nasServer = nasServers[i]!;
      logger.info(`Scanning NAS: ${nasServer} (${i + 1}/${nasServers.length})`);

      try {
        // ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
        const adapter = FileSystemAdapterFactory.createFromEnv();
        const database = new DatabaseManager({
          dbPath: process.env.DB_PATH
        });

        await adapter.connect();
        await database.initialize();

        // NASãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        const nasPath = `${config.scanner.nasPath}/${nasServer}`;

        const scanner = new FileScanner({
          adapter,
          database,
          excludePatterns: config.scanner.excludePatterns,
          maxFileSize: config.scanner.maxFileSize,
          concurrency: config.scanner.parallelism,
          batchSize: config.scanner.batchSize,
          dryRun: config.scanner.dryRun
        });

        const uploader = new S3Uploader({
          awsConfig: config.aws,
          adapter,
          database,
          dryRun: config.scanner.dryRun
        });

        let scanResult;
        if (fullSync) {
          scanResult = await scanner.startScan(nasPath);
        } else {
          const stats = await database.getStatistics();
          if (stats.lastScanTime) {
            scanResult = await scanner.quickScan(nasPath, stats.lastScanTime);
          } else {
            scanResult = await scanner.startScan(nasPath);
          }
        }

        totalNewFiles += scanResult.newFiles.length;
        totalChangedFiles += scanResult.modifiedFiles.length;
        totalDeletedFiles += scanResult.deletedFiles.length;
        totalSyncedFiles += scanResult.newFiles.length + scanResult.modifiedFiles.length;
        totalErrors += scanResult.errors.length;

        // ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        const filesToUpload = [...scanResult.newFiles, ...scanResult.modifiedFiles];
        if (filesToUpload.length > 0) {
          await uploader.uploadBatch(filesToUpload);
        }

        await uploader.cleanup();
        await database.close();
        await adapter.disconnect();

        // é€²æ—ã‚’æ›´æ–°
        if (onProgress) {
          onProgress(i + 1, nasServers.length, nasServer, totalSyncedFiles);
        }

      } catch (error) {
        totalErrors++;
        logger.error(`Failed to scan NAS ${nasServer}:`, error);
      }
    }

    return {
      newFiles: totalNewFiles,
      changedFiles: totalChangedFiles,
      deletedFiles: totalDeletedFiles,
      syncedFiles: totalSyncedFiles,
      errors: totalErrors
    };
  };

  // ä½¿ç”¨ã™ã‚‹ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œé–¢æ•°ã‚’é¸æŠ
  const scanExecutor = usePowerShell ? scanExecutorPowerShell : scanExecutorNodeJs;

  // SQS Consumerã‚’ä½œæˆã—ã¦é–‹å§‹
  const consumer = new SQSConsumer({
    region: config.aws.region,
    queueUrl: syncQueueUrl,
    dynamoTableName,
    scanExecutor,
    credentials: config.aws.accessKeyId && config.aws.secretAccessKey ? {
      accessKeyId: config.aws.accessKeyId,
      secretAccessKey: config.aws.secretAccessKey
    } : undefined
  });

  // Graceful shutdown
  const shutdown = async () => {
    logger.info('Received shutdown signal');
    await consumer.cleanup();
    process.exit(0);
  };

  process.on('SIGINT', shutdown);
  process.on('SIGTERM', shutdown);

  // çµ±è¨ˆæƒ…å ±ã‚’å®šæœŸçš„ã«è¡¨ç¤º
  setInterval(() => {
    const stats = consumer.getStatistics();
    logger.info(`Consumer stats: processed=${stats.processedCount}, errors=${stats.errorCount}`);
  }, 60000); // 1åˆ†ã”ã¨

  // Consumeré–‹å§‹
  await consumer.start();
}

// CLIã‚³ãƒãƒ³ãƒ‰ã‚’è¨­å®š
program
  .name('cis-file-scanner')
  .description('CIS File Scanner - NAS to S3 file synchronization')
  .version('1.0.0');

// ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã‚³ãƒãƒ³ãƒ‰
program
  .command('scan')
  .description('Run a full scan of the NAS')
  .option('-d, --dry-run', 'Perform a dry run without uploading')
  .option('--skip-sqs', 'Skip SQS message publishing')
  .action(runFullScan);

// å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³ã‚³ãƒãƒ³ãƒ‰
program
  .command('diff')
  .description('Run a differential scan (only changed files)')
  .option('-d, --dry-run', 'Perform a dry run without uploading')
  .action(runDifferentialScan);

// ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
program
  .command('schedule <cron>')
  .description('Start scheduled scanning (e.g., "0 */6 * * *" for every 6 hours)')
  .action(startScheduledScan);

// çµ±è¨ˆè¡¨ç¤ºã‚³ãƒãƒ³ãƒ‰
program
  .command('stats')
  .description('Show scanning statistics')
  .action(showStatistics);

// SQSè¨ºæ–­ã‚³ãƒãƒ³ãƒ‰
program
  .command('diagnose-sqs')
  .description('Diagnose SQS queue status and performance')
  .action(diagnoseSQS);

// SQS Consumerã‚³ãƒãƒ³ãƒ‰ï¼ˆãƒ‡ãƒ¼ãƒ¢ãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼‰
program
  .command('consumer')
  .description('Start SQS consumer mode to process sync requests from web UI')
  .action(startSQSConsumer);

// CLIã‚’å®Ÿè¡Œ
program.parse();

// ã‚³ãƒãƒ³ãƒ‰ãŒæŒ‡å®šã•ã‚Œãªã‹ã£ãŸå ´åˆ
if (!process.argv.slice(2).length) {
  program.outputHelp();
}