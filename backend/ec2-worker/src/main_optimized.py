#!/usr/bin/env python3
"""
CIS File Search Application - EC2 Worker - OPTIMIZED VERSION
Main entry point for high-performance file processing worker

Performance Target: 500-1000 messages/minute on t3.medium
"""

import logging
import sys
import os
import time
import gc
import psutil
from datetime import datetime
from typing import Dict

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from config_optimized import config
from s3_client import S3Client
from ocr_processor import OCRProcessor
from thumbnail_generator import ThumbnailGenerator
from bedrock_client import BedrockClient
from opensearch_client import OpenSearchClient
from sqs_handler_optimized import OptimizedSQSHandler
from log_filter import SensitiveDataFilter, PathSanitizer, setup_secure_logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
def setup_logging():
    """ãƒ­ã‚®ãƒ³ã‚°ã‚’è¨­å®š"""
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

    # ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼è¨­å®š
    logging.basicConfig(
        level=getattr(logging, config.logging.level),
        format=log_format
    )

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¿½åŠ ï¼ˆè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    if config.logging.file:
        file_handler = logging.FileHandler(config.logging.file)
        file_handler.setFormatter(logging.Formatter(log_format))
        logging.getLogger().addHandler(file_handler)

    # âœ… SECURITY: Add sensitive data filter to all logs
    setup_secure_logging()

logger = logging.getLogger(__name__)


class OptimizedFileProcessor:
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        logger.info("Initializing OPTIMIZED File Processor...")

        # å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        self.s3_client = S3Client()

        # âœ… OPTIMIZATION: æ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿åˆæœŸåŒ–ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
        self.ocr_processor = OCRProcessor() if config.features.enable_ocr else None
        self.thumbnail_generator = ThumbnailGenerator() if config.features.enable_thumbnail else None
        self.bedrock_client = BedrockClient() if config.features.enable_vector_search else None

        self.opensearch_client = OpenSearchClient()

        # å‡¦ç†çµ±è¨ˆ
        self.stats = {
            'processed': 0,
            'failed': 0,
            'start_time': time.time(),
            'last_memory_check': time.time()
        }

        # âœ… OPTIMIZATION: æœ€é©ãªã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’è¨ˆç®—
        optimal_threads = config.get_optimal_thread_count()
        logger.info(f"Optimal thread count: {optimal_threads} (configured: {config.worker.threads})")

        logger.info(f"Features enabled: OCR={config.features.enable_ocr}, "
                   f"Thumbnail={config.features.enable_thumbnail}, "
                   f"Vector={config.features.enable_vector_search}")
        logger.info("File Processor initialized successfully")

    def process_file(self, bucket: str, key: str) -> bool:
        """
        S3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦OpenSearchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        âœ… OPTIMIZATION:
        - ä¸è¦ãªæ©Ÿèƒ½ã‚’ã‚¹ã‚­ãƒƒãƒ—
        - ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡çš„ãªå‡¦ç†
        - æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³ã§ç„¡é§„ãªå‡¦ç†ã‚’å›žé¿

        Args:
            bucket: S3ãƒã‚±ãƒƒãƒˆå
            key: S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼

        Returns:
            å‡¦ç†æˆåŠŸã®å ´åˆTrue
        """
        start_time = time.time()
        temp_file = None

        try:
            # âœ… OPTIMIZATION: ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯ï¼ˆå®šæœŸçš„ï¼‰
            if time.time() - self.stats['last_memory_check'] > config.performance.memory_check_interval:
                self._check_memory_usage()
                self.stats['last_memory_check'] = time.time()

            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—
            file_info = self.s3_client.get_object_metadata(bucket, key)
            if not file_info:
                logger.error(f"File not found: s3://{bucket}/{key}")
                return False

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸºæœ¬æƒ…å ±ã‚’æ§‹ç¯‰
            from pathlib import Path
            import hashlib

            document = {
                'file_id': hashlib.md5(f"{bucket}/{key}".encode()).hexdigest(),
                'file_name': Path(key).name,
                'file_path': f"s3://{bucket}/{key}",
                'file_extension': Path(key).suffix.lower(),
                'file_size': file_info['ContentLength'],
                'mime_type': file_info.get('ContentType', 'application/octet-stream'),
                'modified_at': file_info.get('LastModified', datetime.utcnow()).isoformat(),
                'processing_status': 'processing'
            }

            # âœ… OPTIMIZATION: é‡ã„å‡¦ç†ï¼ˆOCRã€ã‚µãƒ ãƒã‚¤ãƒ«ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½ã«
            # é€Ÿåº¦å„ªå…ˆã®å ´åˆã¯ã™ã¹ã¦ç„¡åŠ¹åŒ–

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯å¿…è¦ãªæ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿
            needs_download = (config.features.enable_ocr or
                            config.features.enable_thumbnail or
                            config.features.enable_vector_search)

            if needs_download:
                temp_file = self.s3_client.download_file(bucket, key)

                # OCRå‡¦ç†ï¼ˆæœ‰åŠ¹ãªå ´åˆã®ã¿ï¼‰
                if config.features.enable_ocr and self.ocr_processor:
                    ocr_result = self._perform_ocr(temp_file, document)
                    if ocr_result:
                        document.update(ocr_result)

                # ã‚µãƒ ãƒã‚¤ãƒ«ç”Ÿæˆï¼ˆæœ‰åŠ¹ãªå ´åˆã®ã¿ï¼‰
                if config.features.enable_thumbnail and self.thumbnail_generator:
                    thumbnail_result = self._generate_thumbnail(temp_file, key, document)
                    if thumbnail_result:
                        document.update(thumbnail_result)

                # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆæœ‰åŠ¹ãªå ´åˆã®ã¿ï¼‰
                if config.features.enable_vector_search and self.bedrock_client:
                    vector_result = self._generate_vector(temp_file, document)
                    if vector_result:
                        document.update(vector_result)

            # OpenSearchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            document['processing_status'] = 'completed'
            document['indexed_at'] = datetime.utcnow().isoformat()

            success = self.opensearch_client.index_document(document)

            if success:
                # S3ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚±ãƒƒãƒˆã®å ´åˆï¼‰
                if 'landing' in bucket.lower():
                    self.s3_client.delete_file(bucket, key)

                self.stats['processed'] += 1

                # âœ… OPTIMIZATION: å‡¦ç†æ™‚é–“ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ«ã§ãƒ­ã‚°ï¼ˆINFO ã ã¨é…ã„ï¼‰
                processing_time = time.time() - start_time
                logger.debug(f"Processed {key} in {processing_time:.2f}s")

                return True
            else:
                logger.error(f"Failed to index document: {key}")
                self.stats['failed'] += 1
                return False

        except Exception as e:
            logger.error(f"Error processing file {key}: {str(e)}")
            self.stats['failed'] += 1
            return False

        finally:
            # âœ… OPTIMIZATION: ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œ
            if temp_file:
                self.s3_client.cleanup_temp_file(temp_file)

            # âœ… OPTIMIZATION: å®šæœŸçš„ã«ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            if self.stats['processed'] % 100 == 0:
                gc.collect()

    def _perform_ocr(self, file_path: str, document: Dict) -> Dict:
        """OCRå‡¦ç†ã‚’å®Ÿè¡Œï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰"""
        try:
            ocr_result = self.ocr_processor.process_file(file_path)
            if ocr_result['success']:
                return {
                    'ocr_text': ocr_result.get('text', ''),
                    'ocr_confidence': ocr_result.get('confidence', 0.0),
                    'content': ocr_result.get('text', ''),
                    'pages': ocr_result.get('pages', 1)
                }
            return {}
        except Exception as e:
            logger.error(f"OCR processing error: {str(e)}")
            return {}

    def _generate_thumbnail(self, file_path: str, key: str, document: Dict) -> Dict:
        """ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ç”Ÿæˆï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰"""
        try:
            from pathlib import Path
            import io

            thumbnail_data = self.thumbnail_generator.generate_with_metadata(file_path)
            if thumbnail_data['thumbnail']:
                thumbnail_key = f"thumbnails/{Path(key).stem}_thumb.jpg"
                thumbnail_io = io.BytesIO(thumbnail_data['thumbnail'])

                thumbnail_url = self.s3_client.upload_fileobj(
                    thumbnail_io,
                    config.s3.thumbnail_bucket,
                    thumbnail_key,
                    content_type='image/jpeg'
                )

                return {
                    'thumbnail_url': thumbnail_url,
                    'thumbnail_s3_key': thumbnail_key
                }
            return {}
        except Exception as e:
            logger.error(f"Thumbnail generation error: {str(e)}")
            return {}

    def _generate_vector(self, file_path: str, document: Dict) -> Dict:
        """ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰"""
        try:
            from pathlib import Path

            file_extension = Path(file_path).suffix.lower()

            if file_extension in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:
                vector = self.bedrock_client.generate_image_embedding(file_path)
                if vector:
                    return {'image_vector': vector}

            elif document.get('content'):
                text = document['content'][:1000]
                vector = self.bedrock_client.generate_text_embedding(text)
                if vector:
                    return {'image_vector': vector}

            return {}
        except Exception as e:
            logger.error(f"Vector generation error: {str(e)}")
            return {}

    def _check_memory_usage(self):
        """
        âœ… NEW: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦è­¦å‘Š
        """
        try:
            memory = psutil.virtual_memory()
            memory_percent = memory.percent

            if memory_percent > config.performance.memory_warning_threshold:
                logger.warning(f"âš ï¸  High memory usage: {memory_percent:.1f}% "
                             f"(threshold: {config.performance.memory_warning_threshold}%)")

                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ90%ã‚’è¶…ãˆãŸã‚‰å¼·åˆ¶çš„ã«GCå®Ÿè¡Œ
                if memory_percent > 90:
                    logger.warning("Forcing garbage collection due to high memory usage")
                    gc.collect()

        except Exception as e:
            logger.error(f"Failed to check memory usage: {str(e)}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
    setup_logging()

    logger.info("=" * 80)
    logger.info("CIS File Search Application - EC2 Worker [OPTIMIZED VERSION]")
    logger.info(f"Version: 2.0.0 (Performance Optimized)")
    logger.info(f"Target: {config.performance.target_messages_per_minute} messages/minute")
    logger.info(f"Region: {config.aws.region}")
    logger.info(f"Queue: {config.sqs.queue_url}")
    logger.info(f"Worker Threads: {config.worker.threads}")
    logger.info(f"Parallel SQS Fetch: {config.sqs.parallel_fetch_count}")
    logger.info("=" * 80)

    # è¨­å®šæ¤œè¨¼
    if not config.validate():
        logger.error("Configuration validation failed")
        sys.exit(1)

    try:
        # âœ… OPTIMIZATION: ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¡¨ç¤º
        cpu_count = psutil.cpu_count()
        memory_total_gb = psutil.virtual_memory().total / (1024**3)
        logger.info(f"System Resources: {cpu_count} CPUs, {memory_total_gb:.1f}GB RAM")

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’åˆæœŸåŒ–
        processor = OptimizedFileProcessor()

        # æŽ¥ç¶šãƒ†ã‚¹ãƒˆ
        logger.info("Testing connections...")

        # Bedrockãƒ†ã‚¹ãƒˆï¼ˆæœ‰åŠ¹ãªå ´åˆã®ã¿ï¼‰
        if config.features.enable_vector_search and processor.bedrock_client:
            if not processor.bedrock_client.test_connection():
                logger.warning("Bedrock connection test failed, vector search disabled")
                config.features.enable_vector_search = False

        # OpenSearchãƒ†ã‚¹ãƒˆ
        stats = processor.opensearch_client.get_stats()
        logger.info(f"OpenSearch index contains {stats.get('document_count', 0)} documents")

        # âœ… OPTIMIZATION: æœ€é©åŒ–ã•ã‚ŒãŸSQSãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ä½¿ç”¨
        sqs_handler = OptimizedSQSHandler(processor.process_file)

        # ã‚­ãƒ¥ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
        queue_depth = sqs_handler.get_queue_depth()
        logger.info(f"Queue depth: {queue_depth} messages")

        if queue_depth > 0:
            estimated_hours = (queue_depth / config.performance.target_messages_per_minute) / 60
            logger.info(f"Estimated completion time: {estimated_hours:.1f} hours "
                       f"(at {config.performance.target_messages_per_minute} msg/min)")

        # ãƒãƒ¼ãƒªãƒ³ã‚°é–‹å§‹
        logger.info("ðŸš€ Starting OPTIMIZED message processing...")
        sqs_handler.start_polling()

    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt, shutting down...")

    except Exception as e:
        logger.error(f"Fatal error: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)

    finally:
        # æœ€çµ‚çµ±è¨ˆã‚’è¡¨ç¤º
        if 'processor' in locals():
            uptime = time.time() - processor.stats['start_time']
            logger.info("=" * 80)
            logger.info("Final Statistics:")
            logger.info(f"Processed: {processor.stats['processed']} files")
            logger.info(f"Failed: {processor.stats['failed']} files")
            logger.info(f"Uptime: {uptime:.2f} seconds ({uptime/60:.1f} minutes)")

            if uptime > 0:
                msg_per_min = (processor.stats['processed'] / uptime) * 60
                logger.info(f"Average Speed: {msg_per_min:.1f} messages/minute")

            logger.info("=" * 80)

    logger.info("Worker shutdown complete")
    sys.exit(0)


if __name__ == "__main__":
    main()
