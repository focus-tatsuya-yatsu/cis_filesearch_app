"""
SQS Message Handler for File Processing - OPTIMIZED VERSION
Performance Target: 500-1000 messages/minute

æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ:
1. ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ + ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†
2. ãƒãƒƒãƒå‡¦ç†ã®åŠ¹ç‡åŒ–ï¼ˆè¤‡æ•°SQSå‘¼ã³å‡ºã—ã®ä¸¦åˆ—åŒ–ï¼‰
3. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„ï¼ˆå‡¦ç†å®Œäº†å¾Œã®å³æ™‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
4. å‹•çš„ãªVisibilityTimeoutèª¿æ•´
5. å‡¦ç†é€Ÿåº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¯è¦–åŒ–
"""

import logging
import json
import time
import signal
import os
from typing import Optional, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import boto3
from botocore.exceptions import ClientError
from config import config

logger = logging.getLogger(__name__)


class OptimizedSQSHandler:
    """æœ€é©åŒ–ã•ã‚ŒãŸSQSãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""

    def __init__(self, message_processor):
        """
        åˆæœŸåŒ–

        Args:
            message_processor: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã™ã‚‹é–¢æ•°
        """
        self.sqs = boto3.client('sqs', **config.get_boto3_config())
        self.queue_url = config.sqs.queue_url
        self.visibility_timeout = config.sqs.visibility_timeout
        self.max_messages = config.sqs.max_messages
        self.message_processor = message_processor

        # âœ… æœ€é©åŒ–1: CPUæ•°ã«åŸºã¥ãå‹•çš„ã‚¹ãƒ¬ãƒƒãƒ‰æ•°è¨­å®š
        # t3.medium (2vCPU, 4GB RAM) ã®å ´åˆ: 8-12ã‚¹ãƒ¬ãƒƒãƒ‰æ¨å¥¨
        cpu_cores = cpu_count()
        optimal_threads = max(config.worker.threads, cpu_cores * 4)  # CPUã‚³ã‚¢æ•° x 4

        # ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’è€ƒæ…®ï¼ˆ4GB RAM = 1ã‚¹ãƒ¬ãƒƒãƒ‰ã‚ãŸã‚Šç´„300MBä½¿ç”¨å¯èƒ½ï¼‰
        max_safe_threads = 12  # 4GB / 300MB â‰ˆ 13, å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã§12
        self.thread_count = min(optimal_threads, max_safe_threads)

        logger.info(f"Using {self.thread_count} worker threads (CPU cores: {cpu_cores})")

        # âœ… æœ€é©åŒ–2: ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«è¨­å®šï¼ˆè¤‡æ•°ãƒãƒƒãƒã®ä¸¦åˆ—å‡¦ç†ç”¨ï¼‰
        self.executor = ThreadPoolExecutor(max_workers=self.thread_count)

        # âœ… æœ€é©åŒ–3: ä¸¦åˆ—SQSå—ä¿¡ç”¨ã®è¿½åŠ ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«
        self.sqs_fetch_executor = ThreadPoolExecutor(max_workers=3)

        # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ•ãƒ©ã‚°
        self.shutdown_requested = False

        # âœ… æœ€é©åŒ–4: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        self.performance_stats = {
            'total_processed': 0,
            'total_failed': 0,
            'start_time': time.time(),
            'batch_times': [],
            'messages_per_minute': []
        }

        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®šï¼ˆSpotä¸­æ–­å¯¾å¿œï¼‰
        signal.signal(signal.SIGTERM, self._handle_shutdown_signal)
        signal.signal(signal.SIGINT, self._handle_shutdown_signal)

        logger.info(f"Initialized OPTIMIZED SQS handler for queue: {self.queue_url}")

    def start_polling(self):
        """
        SQSãƒãƒ¼ãƒªãƒ³ã‚°ã‚’é–‹å§‹ï¼ˆæœ€é©åŒ–ç‰ˆãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ï¼‰

        âœ… æœ€é©åŒ–æˆ¦ç•¥:
        - è¤‡æ•°ã®SQSå—ä¿¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œ
        - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹é™ã‚Šå¾…æ©Ÿæ™‚é–“ãªã—ã§é€£ç¶šå‡¦ç†
        - å‡¦ç†é€Ÿåº¦ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
        """
        logger.info("Starting OPTIMIZED SQS polling...")
        logger.info(f"Target: 500-1000 messages/minute")

        consecutive_empty_batches = 0
        last_stats_time = time.time()

        while not self.shutdown_requested:
            try:
                batch_start = time.time()

                # âœ… æœ€é©åŒ–5: è¤‡æ•°ã®SQSå—ä¿¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œ
                # 3ã¤ã®å—ä¿¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åŒæ™‚ã«é€ä¿¡ã—ã€ã‚ˆã‚Šå¤šãã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
                message_batches = self._receive_messages_parallel(num_batches=3)

                total_messages = sum(len(batch) for batch in message_batches)

                if total_messages > 0:
                    logger.info(f"ğŸ“¥ Received {total_messages} messages (from {len(message_batches)} parallel requests)")
                    consecutive_empty_batches = 0

                    # âœ… æœ€é©åŒ–6: å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸€æ‹¬ä¸¦åˆ—å‡¦ç†
                    all_messages = []
                    for batch in message_batches:
                        all_messages.extend(batch)

                    self._process_messages_optimized(all_messages)

                    # ãƒãƒƒãƒå‡¦ç†æ™‚é–“ã‚’è¨˜éŒ²
                    batch_duration = time.time() - batch_start
                    self.performance_stats['batch_times'].append(batch_duration)

                    # âœ… æœ€é©åŒ–7: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹é™ã‚Šå¾…æ©Ÿãªã—ã§æ¬¡ã®å‡¦ç†ã¸
                    # ã‚­ãƒ¥ãƒ¼ãŒç©ºã«ãªã‚‹ã¾ã§é«˜é€Ÿå‡¦ç†ã‚’ç¶™ç¶š
                    continue

                else:
                    consecutive_empty_batches += 1

                    # âœ… æœ€é©åŒ–8: å‹•çš„å¾…æ©Ÿæ™‚é–“
                    # ç©ºãƒãƒƒãƒãŒé€£ç¶šã™ã‚‹å ´åˆã®ã¿å¾…æ©Ÿæ™‚é–“ã‚’å¢—ã‚„ã™
                    if consecutive_empty_batches >= 3:
                        # ã‚­ãƒ¥ãƒ¼ãŒç©ºã®å ´åˆã¯é•·ã‚ã«å¾…æ©Ÿï¼ˆãƒªã‚½ãƒ¼ã‚¹ç¯€ç´„ï¼‰
                        wait_time = min(20, consecutive_empty_batches * 5)
                        logger.debug(f"Queue empty, waiting {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        # ã¾ã ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§çŸ­æ™‚é–“å¾…æ©Ÿ
                        time.sleep(1)

                # âœ… æœ€é©åŒ–9: 30ç§’ã”ã¨ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’è¡¨ç¤º
                if time.time() - last_stats_time >= 30:
                    self._log_performance_stats()
                    last_stats_time = time.time()

            except KeyboardInterrupt:
                logger.info("Received keyboard interrupt")
                self.shutdown()
                break

            except Exception as e:
                logger.error(f"Error in polling loop: {str(e)}")
                time.sleep(5)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯çŸ­ã‚ã®å¾…æ©Ÿ

        logger.info("SQS polling stopped")

    def _receive_messages_parallel(self, num_batches: int = 3) -> List[List[Dict]]:
        """
        è¤‡æ•°ã®SQSå—ä¿¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œ

        âœ… æœ€é©åŒ–: Long Pollingã‚’ä½¿ã„ã¤ã¤ã€è¤‡æ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æœ€å¤§30ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å–å¾—
        ï¼ˆ1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šæœ€å¤§10ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ x 3ãƒªã‚¯ã‚¨ã‚¹ãƒˆ = 30ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰

        Args:
            num_batches: ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°

        Returns:
            ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒƒãƒã®ãƒªã‚¹ãƒˆ
        """
        futures = []

        # è¤‡æ•°ã®å—ä¿¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸¦åˆ—é€ä¿¡
        for _ in range(num_batches):
            future = self.sqs_fetch_executor.submit(self._receive_messages)
            futures.append(future)

        # å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®çµæœã‚’åé›†
        message_batches = []
        for future in as_completed(futures):
            try:
                messages = future.result(timeout=25)  # Long Polling timeout + ãƒãƒƒãƒ•ã‚¡
                if messages:
                    message_batches.append(messages)
            except Exception as e:
                logger.error(f"Failed to receive messages: {str(e)}")

        return message_batches

    def _receive_messages(self) -> List[Dict]:
        """
        SQSã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡ï¼ˆå˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰

        Returns:
            ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
        """
        try:
            response = self.sqs.receive_message(
                QueueUrl=self.queue_url,
                MaxNumberOfMessages=self.max_messages,
                VisibilityTimeout=self.visibility_timeout,
                WaitTimeSeconds=20,  # Long Pollingï¼ˆ20ç§’å¾…æ©Ÿï¼‰
                MessageAttributeNames=['All']
            )

            messages = response.get('Messages', [])
            return messages

        except ClientError as e:
            logger.error(f"Failed to receive messages from SQS: {str(e)}")
            return []

    def _process_messages_optimized(self, messages: List[Dict]):
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æœ€é©åŒ–ã•ã‚ŒãŸæ–¹æ³•ã§ä¸¦åˆ—å‡¦ç†

        âœ… æœ€é©åŒ–:
        - å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å³åº§ã«ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã«æŠ•å…¥
        - å‡¦ç†å®Œäº†ã‚’å¾…ãŸãšæ¬¡ã®ãƒãƒƒãƒå–å¾—ã¸ï¼ˆéåŒæœŸçš„ãªå‡¦ç†ï¼‰
        - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸå‹•çš„ãªåŒæ™‚å®Ÿè¡Œæ•°åˆ¶å¾¡

        Args:
            messages: SQSãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
        """
        futures = []

        # âœ… æœ€é©åŒ–10: å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸€æ‹¬ã§ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã«æŠ•å…¥
        for message in messages:
            future = self.executor.submit(self._process_single_message, message)
            futures.append((future, message))

        # âœ… æœ€é©åŒ–11: éåŒæœŸçš„ãªçµæœå‡¦ç†
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’çŸ­ãè¨­å®šã—ã€å‡¦ç†å®Œäº†æ¬¡ç¬¬æ¬¡ã®å‡¦ç†ã¸
        for future, message in futures:
            try:
                # å‹•çš„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆå¹³å‡å‡¦ç†æ™‚é–“ã®2å€ã¾ã§å¾…æ©Ÿï¼‰
                avg_processing_time = self._get_average_processing_time()
                timeout = min(avg_processing_time * 2, self.visibility_timeout - 10)

                success = future.result(timeout=timeout)

                if success:
                    # å‡¦ç†æˆåŠŸï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å³åº§ã«å‰Šé™¤
                    self._delete_message(message)
                    self.performance_stats['total_processed'] += 1
                else:
                    # å‡¦ç†å¤±æ•—ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯è‡ªå‹•çš„ã«å†è¡¨ç¤ºã•ã‚Œã‚‹
                    logger.warning(f"Message processing failed, will be retried")
                    self.performance_stats['total_failed'] += 1

            except Exception as e:
                logger.error(f"Error processing message: {str(e)}")
                self.performance_stats['total_failed'] += 1

    def _process_single_message(self, message: Dict) -> bool:
        """
        å˜ä¸€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†

        Args:
            message: SQSãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Returns:
            å‡¦ç†æˆåŠŸã®å ´åˆTrue
        """
        try:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æœ¬æ–‡ã‚’è§£æ
            body = json.loads(message['Body'])

            # EventBridgeçµŒç”±ã®S3ã‚¤ãƒ™ãƒ³ãƒˆã®å ´åˆ
            if 'detail' in body:
                return self._process_s3_event(body['detail'])

            # ç›´æ¥ã®S3ã‚¤ãƒ™ãƒ³ãƒˆã®å ´åˆ
            elif 'Records' in body:
                for record in body['Records']:
                    if record.get('eventSource') == 'aws:s3':
                        return self._process_s3_record(record)

            # ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å ´åˆ
            else:
                return self._process_custom_message(body)

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse message body: {str(e)}")
            return False

        except Exception as e:
            logger.error(f"Failed to process message: {str(e)}")
            return False

    def _process_s3_event(self, detail: Dict) -> bool:
        """
        EventBridgeçµŒç”±ã®S3ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†

        Args:
            detail: ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°

        Returns:
            å‡¦ç†æˆåŠŸã®å ´åˆTrue
        """
        try:
            bucket = detail.get('bucket', {}).get('name')
            key = detail.get('object', {}).get('key')

            if not bucket or not key:
                logger.error("Missing bucket or key in S3 event")
                return False

            logger.debug(f"Processing S3 file: s3://{bucket}/{key}")

            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’å®Ÿè¡Œ
            return self.message_processor(bucket, key)

        except Exception as e:
            logger.error(f"Failed to process S3 event: {str(e)}")
            return False

    def _process_s3_record(self, record: Dict) -> bool:
        """
        S3ã‚¤ãƒ™ãƒ³ãƒˆãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‡¦ç†

        Args:
            record: S3ã‚¤ãƒ™ãƒ³ãƒˆãƒ¬ã‚³ãƒ¼ãƒ‰

        Returns:
            å‡¦ç†æˆåŠŸã®å ´åˆTrue
        """
        try:
            bucket = record['s3']['bucket']['name']
            key = record['s3']['object']['key']

            logger.debug(f"Processing S3 file: s3://{bucket}/{key}")

            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’å®Ÿè¡Œ
            return self.message_processor(bucket, key)

        except Exception as e:
            logger.error(f"Failed to process S3 record: {str(e)}")
            return False

    def _process_custom_message(self, body: Dict) -> bool:
        """
        ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†

        Args:
            body: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æœ¬æ–‡

        Returns:
            å‡¦ç†æˆåŠŸã®å ´åˆTrue
        """
        try:
            message_type = body.get('type')

            if message_type == 'sync_start':
                logger.info("Received sync start message")
                return True

            elif message_type == 'reindex':
                logger.info("Received reindex message")
                file_path = body.get('file_path')
                if file_path:
                    logger.info(f"Reindexing file: {file_path}")
                    return True
                return False

            else:
                logger.warning(f"Unknown message type: {message_type}")
                return True  # ä¸æ˜ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å‰Šé™¤

        except Exception as e:
            logger.error(f"Failed to process custom message: {str(e)}")
            return False

    def _delete_message(self, message: Dict):
        """
        å‡¦ç†æ¸ˆã¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤

        Args:
            message: SQSãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        try:
            self.sqs.delete_message(
                QueueUrl=self.queue_url,
                ReceiptHandle=message['ReceiptHandle']
            )

        except ClientError as e:
            logger.error(f"Failed to delete message: {str(e)}")

    def _get_average_processing_time(self) -> float:
        """
        å¹³å‡å‡¦ç†æ™‚é–“ã‚’å–å¾—

        Returns:
            å¹³å‡å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰
        """
        if not self.performance_stats['batch_times']:
            return 60.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

        # ç›´è¿‘10ãƒãƒƒãƒã®å¹³å‡ã‚’è¨ˆç®—
        recent_batches = self.performance_stats['batch_times'][-10:]
        avg_batch_time = sum(recent_batches) / len(recent_batches)

        # ãƒãƒƒãƒã‚ãŸã‚Šã®å¹³å‡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã§å‰²ã‚‹
        avg_messages_per_batch = 10  # ä»®å®š
        return avg_batch_time / avg_messages_per_batch

    def _log_performance_stats(self):
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›

        âœ… æœ€é©åŒ–12: å‡¦ç†é€Ÿåº¦ã®å¯è¦–åŒ–ã§å•é¡Œã‚’æ—©æœŸç™ºè¦‹
        """
        elapsed_time = time.time() - self.performance_stats['start_time']
        total_processed = self.performance_stats['total_processed']
        total_failed = self.performance_stats['total_failed']

        # åˆ†ã‚ãŸã‚Šã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—
        messages_per_minute = (total_processed / elapsed_time) * 60 if elapsed_time > 0 else 0

        # æ™‚é–“ã‚ãŸã‚Šã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°
        messages_per_hour = messages_per_minute * 60

        # æˆåŠŸç‡
        total_attempts = total_processed + total_failed
        success_rate = (total_processed / total_attempts * 100) if total_attempts > 0 else 0

        logger.info("=" * 80)
        logger.info("ğŸ“Š PERFORMANCE STATISTICS")
        logger.info(f"â±ï¸  Uptime: {elapsed_time:.0f}s ({elapsed_time/60:.1f}m)")
        logger.info(f"âœ… Processed: {total_processed} messages")
        logger.info(f"âŒ Failed: {total_failed} messages")
        logger.info(f"ğŸ“ˆ Success Rate: {success_rate:.1f}%")
        logger.info(f"ğŸš€ Speed: {messages_per_minute:.1f} msg/min ({messages_per_hour:.0f} msg/hour)")

        # ç›®æ¨™é”æˆçŠ¶æ³
        if messages_per_minute >= 500:
            logger.info(f"ğŸ¯ TARGET ACHIEVED! Current: {messages_per_minute:.0f} msg/min >= 500 msg/min")
        else:
            remaining = 500 - messages_per_minute
            logger.info(f"ğŸ¯ Target: 500 msg/min (Current: {messages_per_minute:.0f}, Need: +{remaining:.0f})")

        # æ®‹ã‚Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã¨äºˆæƒ³å®Œäº†æ™‚é–“
        queue_depth = self.get_queue_depth()
        if queue_depth > 0 and messages_per_minute > 0:
            estimated_hours = (queue_depth / messages_per_minute) / 60
            logger.info(f"ğŸ“¦ Queue Depth: {queue_depth} messages")
            logger.info(f"â³ Estimated Completion: {estimated_hours:.1f} hours")

        logger.info("=" * 80)

        # çµ±è¨ˆã‚’è¨˜éŒ²
        self.performance_stats['messages_per_minute'].append(messages_per_minute)

    def _handle_shutdown_signal(self, signum, frame):
        """
        ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚·ã‚°ãƒŠãƒ«ã‚’å‡¦ç†ï¼ˆSpotä¸­æ–­å¯¾å¿œï¼‰

        Args:
            signum: ã‚·ã‚°ãƒŠãƒ«ç•ªå·
            frame: ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        logger.info(f"Received shutdown signal: {signum}")
        self.shutdown()

    def shutdown(self):
        """
        ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
        """
        logger.info("Initiating graceful shutdown...")
        self.shutdown_requested = True

        # æœ€çµ‚çµ±è¨ˆã‚’è¡¨ç¤º
        self._log_performance_stats()

        # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
        logger.info("Shutting down thread pools...")
        self.executor.shutdown(wait=True)
        self.sqs_fetch_executor.shutdown(wait=True)

        logger.info("Shutdown complete")

    def get_queue_attributes(self) -> Dict:
        """
        ã‚­ãƒ¥ãƒ¼å±æ€§ã‚’å–å¾—

        Returns:
            ã‚­ãƒ¥ãƒ¼å±æ€§ã®è¾æ›¸
        """
        try:
            response = self.sqs.get_queue_attributes(
                QueueUrl=self.queue_url,
                AttributeNames=['All']
            )
            return response.get('Attributes', {})

        except ClientError as e:
            logger.error(f"Failed to get queue attributes: {str(e)}")
            return {}

    def get_queue_depth(self) -> int:
        """
        ã‚­ãƒ¥ãƒ¼ã®æ·±ã•ï¼ˆå¾…æ©Ÿãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ï¼‰ã‚’å–å¾—

        Returns:
            å¾…æ©Ÿãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°
        """
        attributes = self.get_queue_attributes()
        visible = int(attributes.get('ApproximateNumberOfMessages', 0))
        not_visible = int(attributes.get('ApproximateNumberOfMessagesNotVisible', 0))
        delayed = int(attributes.get('ApproximateNumberOfMessagesDelayed', 0))

        total = visible + not_visible + delayed
        return total
