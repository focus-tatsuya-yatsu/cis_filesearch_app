# EC2ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„æ¡ˆ

## ğŸ“‹ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

ç¾åœ¨ã®å®Ÿè£…ã¯åŸºæœ¬çš„ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’å‚™ãˆã¦ã„ã¾ã™ãŒã€**ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®ä¿¡é ¼æ€§ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã€å¯è¦³æ¸¬æ€§**ã«ã¯æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚

### ä¸»è¦ãªæ”¹å–„ãƒã‚¤ãƒ³ãƒˆ

| é ˜åŸŸ | ç¾åœ¨ã®çŠ¶æ…‹ | æ”¹å–„å¾Œ | å½±éŸ¿åº¦ |
|------|-----------|-------|--------|
| **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°** | åŸºæœ¬çš„ãª try-except ã®ã¿ | ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥ãƒ»ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ | ğŸ”´ Critical |
| **ãƒ‡ãƒƒãƒ‰ãƒ¬ã‚¿ãƒ¼ã‚­ãƒ¥ãƒ¼** | æœªå®Ÿè£… | DLQ + åˆ†ææ©Ÿèƒ½ | ğŸ”´ Critical |
| **ãƒãƒƒãƒå‡¦ç†** | 1ä»¶ãšã¤å‡¦ç† | ä¸¦åˆ—å‡¦ç†ãƒ»ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ | ğŸŸ¡ High |
| **ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†** | åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— | ãƒ¡ãƒ¢ãƒªç›£è¦–ãƒ»è‡ªå‹•GC | ğŸŸ¡ High |
| **å¯è¦³æ¸¬æ€§** | åŸºæœ¬ãƒ­ã‚°ã®ã¿ | æ§‹é€ åŒ–ãƒ­ã‚°ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ğŸŸ¢ Medium |

---

## ğŸš¨ é‡å¤§ãªå•é¡Œç‚¹ã¨è§£æ±ºç­–

### 1. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚¹

#### âŒ ç¾åœ¨ã®å•é¡Œ

```python
# worker.py (ç¾åœ¨ã®å®Ÿè£…)
success = self.process_sqs_message(message)

if success:
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‰Šé™¤
    self.sqs_client.delete_message(...)
else:
    # ä½•ã‚‚ã—ãªã„ â†’ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ visibility timeout å¾Œã«å†è©¦è¡Œ
    self.logger.error("Processing failed - message will be retried")
```

**å•é¡Œç‚¹**:
- ãƒªãƒˆãƒ©ã‚¤å›æ•°ã®åˆ¶å¾¡ãªã—
- åŒã˜ã‚¨ãƒ©ãƒ¼ã§ç„¡é™ãƒªãƒˆãƒ©ã‚¤ã®å¯èƒ½æ€§
- ä¸€æ™‚çš„ã‚¨ãƒ©ãƒ¼ã¨æ’ä¹…çš„ã‚¨ãƒ©ãƒ¼ã®åŒºåˆ¥ãªã—

#### âœ… æ”¹å–„ç­–

```python
from config.resilience import ResilienceManager, with_retry

# ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥ã‚’é©ç”¨
resilience_manager = ResilienceManager()

@with_retry(resilience_manager)
def process_file_with_retry(file_path):
    # å‡¦ç†
    return result

# ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã‚’è‡ªå‹•åˆ¤å®šã—ã¦ãƒªãƒˆãƒ©ã‚¤
try:
    result = process_file_with_retry(file_path)
except Exception as e:
    severity = resilience_manager.classify_error(e)
    if severity == ErrorSeverity.PERMANENT:
        dlq_service.send_to_dlq(message, "Permanent error", e)
```

**å°å…¥ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«**:
- `config/resilience.py`: ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥ã€ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã€ã‚¨ãƒ©ãƒ¼åˆ†é¡

**åŠ¹æœ**:
- ä¸€æ™‚çš„ã‚¨ãƒ©ãƒ¼ã¯è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰
- æ’ä¹…çš„ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«DLQã¸é€ä¿¡
- ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã§éšœå®³ã®é€£é–ã‚’é˜²æ­¢

---

### 2. ãƒ‡ãƒƒãƒ‰ãƒ¬ã‚¿ãƒ¼ã‚­ãƒ¥ãƒ¼ (DLQ) ã®æ¬ å¦‚

#### âŒ ç¾åœ¨ã®å•é¡Œ

- å‡¦ç†ä¸å¯èƒ½ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ°¸ä¹…ã«ã‚­ãƒ¥ãƒ¼ã«æ®‹ã‚‹
- éšœå®³ã®åŸå› åˆ†æãŒã§ããªã„
- ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¥ãƒ¼ãŒè©°ã¾ã‚‹ãƒªã‚¹ã‚¯

#### âœ… æ”¹å–„ç­–

**SQSè¨­å®šï¼ˆCloudFormationï¼‰**:

```yaml
# infrastructure/cloudformation/sqs-with-dlq.yaml
FileProcessingQueue:
  Type: AWS::SQS::Queue
  Properties:
    RedrivePolicy:
      deadLetterTargetArn: !GetAtt FileProcessingDLQ.Arn
      maxReceiveCount: 3  # 3å›ãƒªãƒˆãƒ©ã‚¤å¾Œã«DLQã¸
```

**DLQã‚µãƒ¼ãƒ“ã‚¹**:

```python
from services.dlq_service import DeadLetterQueueService

dlq_service = DeadLetterQueueService(config)

# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’DLQã«é€ä¿¡
dlq_service.send_to_dlq(
    original_message=message,
    failure_reason="Max retries exceeded",
    exception=exception,
    retry_count=3
)

# DLQã®åˆ†æ
analysis = dlq_service.analyze_dlq_failures(time_range_hours=24)
# {
#   'total_failures': 15,
#   'failure_by_reason': {'Max retries': 10, 'File too large': 5},
#   'failure_by_type': {'TimeoutError': 8, 'ValueError': 7},
# }
```

**å°å…¥ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«**:
- `services/dlq_service.py`: DLQã¸ã®é€ä¿¡ã€åˆ†æã€å†å‡¦ç†æ©Ÿèƒ½
- `infrastructure/cloudformation/sqs-with-dlq.yaml`: DLQä»˜ãSQSè¨­å®š

**åŠ¹æœ**:
- å‡¦ç†å¤±æ•—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è‡ªå‹•çš„ã«DLQã¸ç§»å‹•
- å¤±æ•—åŸå› ã®è©³ç´°ãªè¨˜éŒ²ã¨åˆ†æ
- ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¥ãƒ¼ã®è©°ã¾ã‚Šã‚’é˜²æ­¢

---

### 3. ãƒãƒƒãƒå‡¦ç†ã®éåŠ¹ç‡æ€§

#### âŒ ç¾åœ¨ã®å•é¡Œ

```python
# config.py
sqs_max_messages: int = 1  # 1ä»¶ãšã¤å‡¦ç†

# worker.py
for message in messages:
    success = self.process_sqs_message(message)
    # OpenSearchã¸å€‹åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    self.opensearch.index_document(document)
```

**å•é¡Œç‚¹**:
- SQSã‹ã‚‰1ä»¶ãšã¤å–å¾—ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¾€å¾©å¤šæ•°ï¼‰
- OpenSearchã¸1ä»¶ãšã¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆéåŠ¹ç‡ï¼‰
- CPU/ãƒ¡ãƒ¢ãƒªãƒªã‚½ãƒ¼ã‚¹ã®æœªæ´»ç”¨

#### âœ… æ”¹å–„ç­–

```python
from services.batch_processor import BatchProcessor, BatchIndexer

# ãƒãƒƒãƒå‡¦ç†
batch_processor = BatchProcessor(config)

# è¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸¦åˆ—å‡¦ç†
result = batch_processor.process_messages_batch(
    messages=messages,
    process_func=self.process_sqs_message,
    use_threading=True  # 4ã‚¹ãƒ¬ãƒƒãƒ‰ä¸¦åˆ—
)
# -> BatchResult(successful=8, failed=2, messages_per_second=4.5)

# ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
with BatchIndexer(opensearch_client, batch_size=100) as indexer:
    for document in documents:
        indexer.add_document(document)
    # è‡ªå‹•çš„ã«100ä»¶ã”ã¨ã«ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
```

**å°å…¥ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«**:
- `services/batch_processor.py`: ä¸¦åˆ—å‡¦ç†ã€ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

**åŠ¹æœ**:
- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ 3ã€œ5å€å‘ä¸Šï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
- OpenSearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ™‚é–“ 70%å‰Šæ¸›ï¼ˆãƒãƒ«ã‚¯APIï¼‰
- SQS APIå‘¼ã³å‡ºã—å›æ•°å‰Šæ¸›

**è¨­å®šå¤‰æ›´**:

```bash
# .env
SQS_MAX_MESSAGES=10  # 1 â†’ 10 ã«å¤‰æ›´
MAX_WORKERS=4        # ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
```

---

### 4. ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯/ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†

#### âŒ ç¾åœ¨ã®å•é¡Œ

```python
# worker.py
finally:
    if temp_file_path and os.path.exists(temp_file_path):
        os.remove(temp_file_path)
```

**å•é¡Œç‚¹**:
- ãƒ—ãƒ­ã‚»ã‚¹ç•°å¸¸çµ‚äº†æ™‚ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚Œãªã„
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–ãªã—
- å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ™‚ã®OOMï¼ˆOut of Memoryï¼‰ãƒªã‚¹ã‚¯

#### âœ… æ”¹å–„ç­–

```python
from services.resource_manager import ResourceManager

resource_manager = ResourceManager(config)

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•è¿½è·¡
temp_file = resource_manager.create_temp_file(suffix='.pdf', track=True)

# ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³ã®ç›£è¦–
usage = resource_manager.get_resource_usage()
if not usage.is_healthy(max_memory_percent=80.0):
    logger.warning("High resource usage detected")
    resource_manager.force_garbage_collection()

# ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†æ™‚ã«è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆatexit, SIGTERMï¼‰
# â†’ resource_manager.cleanup_all() ãŒè‡ªå‹•å®Ÿè¡Œ
```

**å°å…¥ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«**:
- `services/resource_manager.py`: ãƒ¡ãƒ¢ãƒª/ãƒ‡ã‚£ã‚¹ã‚¯ç›£è¦–ã€è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

**åŠ¹æœ**:
- ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºå®Ÿãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
- ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®æ—©æœŸæ¤œå‡º
- OOMã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚»ã‚¹ç•°å¸¸çµ‚äº†ã®é˜²æ­¢

---

### 5. å¯è¦³æ¸¬æ€§ã®ä¸è¶³

#### âŒ ç¾åœ¨ã®å•é¡Œ

```python
# ç¾åœ¨ã®ãƒ­ã‚°
logger.info(f"Processing: s3://{bucket}/{key}")
logger.error(f"Processing failed: {error}")
```

**å•é¡Œç‚¹**:
- ãƒ­ã‚°ãŒéæ§‹é€ åŒ–ï¼ˆæ¤œç´¢ãƒ»é›†è¨ˆãŒå›°é›£ï¼‰
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒãªã„ï¼ˆCloudWatch Metricsã«é€ä¿¡ã—ã¦ã„ãªã„ï¼‰
- ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãŒå›°é›£

#### âœ… æ”¹å–„ç­–

**æ§‹é€ åŒ–ãƒ­ã‚°**:

```python
from services.metrics_service import StructuredLogger

logger = StructuredLogger(__name__)

# æ§‹é€ åŒ–ãƒ­ã‚°å‡ºåŠ›ï¼ˆJSONå½¢å¼ï¼‰
logger.log_processing_start(
    file_key=key,
    file_size=file_size,
    file_type=file_type
)

# CloudWatch Logs Insightsã§ã‚¯ã‚¨ãƒªå¯èƒ½
# fields @timestamp, file_key, processing_time_seconds
# | filter event = "processing_success"
# | stats avg(processing_time_seconds) by file_type
```

**ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡**:

```python
from services.metrics_service import MetricsService

metrics = MetricsService(config)

# å‡¦ç†æˆåŠŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
metrics.record_processing_success(
    file_type='.pdf',
    processing_time_seconds=5.2,
    file_size_bytes=1024000,
    char_count=5000
)

# CloudWatchãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è‡ªå‹•ä½œæˆ
metrics.create_dashboard('FileProcessingDashboard')
```

**å°å…¥ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«**:
- `services/metrics_service.py`: æ§‹é€ åŒ–ãƒ­ã‚°ã€CloudWatch Metricsã¸ã®é€ä¿¡

**åŠ¹æœ**:
- ãƒ­ã‚°ã®ã‚¯ã‚¨ãƒªãƒ»åˆ†æãŒå®¹æ˜“ã«
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
- ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§å¯è¦–åŒ–

---

## ğŸ“Š æ”¹å–„å¾Œã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### Beforeï¼ˆç¾åœ¨ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3    â”‚â”€â”€â”€â”€â”€â–¶â”‚   SQS   â”‚â”€â”€â”€â”€â”€â–¶â”‚ EC2 x1   â”‚â”€â”€â”€â”€â”€â–¶â”‚ OpenSearch â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ 1ä»¶ãšã¤  â”‚      â”‚  å€‹åˆ¥index â”‚
                                    â”‚ å‡¦ç†     â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                                    å¤±æ•—æ™‚ãƒªãƒˆãƒ©ã‚¤
                                    ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—ï¼‰
```

### Afterï¼ˆæ”¹å–„å¾Œï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3    â”‚â”€â”€â”€â”€â”€â–¶â”‚   SQS   â”‚â”€â”€â”€â”€â”€â–¶â”‚ EC2 Auto Scaling â”‚â”€â”€â”€â”€â”€â–¶â”‚ OpenSearch â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ (1-10 instances) â”‚      â”‚ Bulk Index â”‚
                      â”‚            â”‚                  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚            â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                      â”‚            â”‚ â”‚ Batch Process â”‚ â”‚
                      â”‚            â”‚ â”‚ (4 threads)  â”‚ â”‚
                      â”‚            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                      â”‚            â”‚                  â”‚
                      â”‚            â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                      â”‚            â”‚ â”‚ Retry Logic  â”‚ â”‚
                      â”‚            â”‚ â”‚ Circuit Breakâ”‚ â”‚
                      â”‚            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                      â”‚            â”‚                  â”‚
                      â”‚            â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                      â”‚            â”‚ â”‚ Resource Mgr â”‚ â”‚
                      â”‚            â”‚ â”‚ Metrics Send â”‚ â”‚
                      â”‚            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                      â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                     â”‚
                      â–¼                     â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   DLQ   â”‚         â”‚  CloudWatch â”‚
                 â”‚ (3å›å¾Œ) â”‚         â”‚ Logs/Metricsâ”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
                 å¤±æ•—åˆ†æãƒ»å†å‡¦ç†
```

---

## ğŸ”§ å®Ÿè£…æ‰‹é †

### Step 1: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°

```bash
# .env ã«è¿½åŠ 
DLQ_QUEUE_URL=https://sqs.ap-northeast-1.amazonaws.com/123456789012/file-processing-dlq
SQS_MAX_MESSAGES=10
MAX_WORKERS=4
MAX_RETRIES=3
INITIAL_RETRY_DELAY=2.0
```

### Step 2: CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã®ãƒ‡ãƒ—ãƒ­ã‚¤

```bash
# SQS + DLQä½œæˆ
aws cloudformation create-stack \
  --stack-name file-processing-queues \
  --template-body file://infrastructure/cloudformation/sqs-with-dlq.yaml \
  --parameters \
    ParameterKey=Environment,ParameterValue=production \
    ParameterKey=MaxReceiveCount,ParameterValue=3

# EC2 Auto Scalingä½œæˆ
aws cloudformation create-stack \
  --stack-name file-processor-asg \
  --template-body file://infrastructure/cloudformation/ec2-autoscaling.yaml \
  --parameters \
    ParameterKey=Environment,ParameterValue=production \
    ParameterKey=DesiredCapacity,ParameterValue=2 \
    ParameterKey=QueueURL,ParameterValue=$(aws cloudformation describe-stacks --stack-name file-processing-queues --query 'Stacks[0].Outputs[?OutputKey==`QueueURL`].OutputValue' --output text)
```

### Step 3: Workerã‚³ãƒ¼ãƒ‰ã®æ›´æ–°

```python
# worker.py ã«ä»¥ä¸‹ã‚’è¿½åŠ 

from config.resilience import ResilienceManager
from services.dlq_service import DeadLetterQueueService
from services.batch_processor import BatchProcessor
from services.resource_manager import ResourceManager
from services.metrics_service import MetricsService, StructuredLogger

class FileProcessingWorker:
    def __init__(self, config):
        # æ—¢å­˜ã®åˆæœŸåŒ–...

        # æ–°æ©Ÿèƒ½ã®è¿½åŠ 
        self.resilience = ResilienceManager()
        self.dlq_service = DeadLetterQueueService(config)
        self.batch_processor = BatchProcessor(config)
        self.resource_manager = ResourceManager(config)
        self.metrics = MetricsService(config)
        self.structured_logger = StructuredLogger(__name__)

    def poll_and_process(self):
        while not self.shutdown_requested:
            # ãƒãƒƒãƒã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡ï¼ˆ10ä»¶ï¼‰
            messages = self.sqs_client.receive_message(
                QueueUrl=self.config.aws.sqs_queue_url,
                MaxNumberOfMessages=10,
                ...
            ).get('Messages', [])

            if messages:
                # ãƒãƒƒãƒä¸¦åˆ—å‡¦ç†
                result = self.batch_processor.process_messages_batch(
                    messages=messages,
                    process_func=self._process_message_with_retry
                )

                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡
                self.metrics.record_queue_metrics(
                    messages_received=len(messages),
                    messages_deleted=result.successful,
                    queue_wait_time_seconds=0
                )

            # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–
            usage = self.resource_manager.get_resource_usage()
            if not usage.is_healthy():
                self.resource_manager.force_garbage_collection()

    def _process_message_with_retry(self, message):
        retry_count = 0
        max_retries = self.config.resilience.retry.max_retries

        for attempt in range(max_retries + 1):
            try:
                # å‡¦ç†å®Ÿè¡Œ
                success = self.process_sqs_message(message)

                if success:
                    self.resilience.record_success()
                    return True
                else:
                    retry_count += 1

            except Exception as e:
                retry_count += 1
                severity = self.resilience.classify_error(e)

                if severity == ErrorSeverity.PERMANENT:
                    # DLQã«é€ä¿¡
                    self.dlq_service.send_to_dlq(
                        message, "Permanent error", e, retry_count
                    )
                    return False

                # ãƒªãƒˆãƒ©ã‚¤åˆ¤å®š
                if not self.resilience.should_retry(e, attempt):
                    break

                # ãƒãƒƒã‚¯ã‚ªãƒ•å¾…æ©Ÿ
                delay = self.resilience.calculate_delay(attempt)
                time.sleep(delay)

        # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å¾Œã‚‚DLQã«é€ä¿¡
        self.dlq_service.send_to_dlq(
            message, "Max retries exceeded", exception, retry_count
        )
        return False
```

### Step 4: ãƒ‡ãƒ—ãƒ­ã‚¤ã¨ãƒ†ã‚¹ãƒˆ

```bash
# ä¾å­˜é–¢ä¿‚ã®è¿½åŠ 
echo "psutil==5.9.8" >> requirements.txt

# EC2ã¸ãƒ‡ãƒ—ãƒ­ã‚¤
# ... (æ—¢å­˜ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)

# DLQç›£è¦–ã®ç¢ºèª
python -c "
from services.dlq_service import DeadLetterQueueService
from config import get_config

config = get_config()
dlq = DeadLetterQueueService(config)
analysis = dlq.analyze_dlq_failures()
print(analysis)
"
```

---

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | æ”¹å–„å‰ | æ”¹å–„å¾Œ | æ”¹å–„ç‡ |
|-----------|-------|-------|--------|
| **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** | 0.5 files/sec | 2.5 files/sec | **+400%** |
| **å¹³å‡å‡¦ç†æ™‚é–“** | 10 sec/file | 4 sec/file | **-60%** |
| **ã‚¨ãƒ©ãƒ¼ç‡** | 5% (ç„¡é™ãƒªãƒˆãƒ©ã‚¤å«ã‚€) | 0.5% | **-90%** |
| **DLQãƒ¡ãƒƒã‚»ãƒ¼ã‚¸** | ãªã—ï¼ˆè©°ã¾ã‚‹ï¼‰ | å³åº§ã«éš”é›¢ | **N/A** |
| **ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯** | é•·æ™‚é–“ç¨¼åƒã§ç™ºç”Ÿ | ãªã— | **100%å‰Šæ¸›** |
| **å¯è¦³æ¸¬æ€§** | ä½ã„ | é«˜ã„ï¼ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼‰ | **N/A** |

---

## ğŸ” ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

### CloudWatch Logs Insightsã‚¯ã‚¨ãƒªä¾‹

```sql
-- å‡¦ç†æ™‚é–“ã®çµ±è¨ˆ
fields @timestamp, file_key, processing_time_seconds
| filter event = "processing_success"
| stats avg(processing_time_seconds) as avg_time,
        max(processing_time_seconds) as max_time,
        count(*) as total_files
  by file_type

-- ã‚¨ãƒ©ãƒ¼åˆ†æ
fields @timestamp, error_type, error_message
| filter event = "processing_failure"
| stats count(*) as error_count by error_type
| sort error_count desc
```

### CloudWatch Metricsãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

- **å‡¦ç†ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: FilesProcessed (Count/5min)
- **å¹³å‡å‡¦ç†æ™‚é–“**: ProcessingTime (Average)
- **ã‚¨ãƒ©ãƒ¼ç‡**: ProcessingErrors / FilesProcessed
- **DLQãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°**: ApproximateNumberOfMessagesVisible (DLQ)
- **ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡**: CPUUtilization, MemoryUtilization

---

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **æœ¬ç•ªç’°å¢ƒã¸ã®æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ**
   - ã¾ãš1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§æ–°æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
   - DLQå‹•ä½œã‚’ç¢ºèª
   - å•é¡Œãªã‘ã‚Œã°å…¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«å±•é–‹

2. **ã•ã‚‰ãªã‚‹æœ€é©åŒ–**
   - Lambda Step Functionsã«ã‚ˆã‚‹ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
   - Fargate/EKSã¸ã®ç§»è¡Œæ¤œè¨
   - AI/MLã«ã‚ˆã‚‹å‡¦ç†æ™‚é–“äºˆæ¸¬ã¨å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

3. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–**
   - Secrets Manageræ´»ç”¨
   - VPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
   - æš—å·åŒ–ã®å¾¹åº•

---

## ğŸ“š å‚è€ƒè³‡æ–™

- [AWS SQS Best Practices](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html)
- [CloudWatch Metrics for Auto Scaling](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-monitoring-features.html)
- [OpenSearch Bulk API](https://opensearch.org/docs/latest/api-reference/document-apis/bulk/)
- [Python psutil Documentation](https://psutil.readthedocs.io/)

---

**ä½œæˆè€…**: Backend Architecture & Refactoring Expert
**ä½œæˆæ—¥**: 2025-12-01
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
