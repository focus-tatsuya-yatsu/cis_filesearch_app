#!/bin/bash
# æœ¬æ¥ã®å‡¦ç†æ©Ÿèƒ½ã‚’æ®µéšçš„ã«å¾©å…ƒã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

set -e

echo "ğŸ”§ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ©Ÿèƒ½ã®å¾©å…ƒ"
echo "========================="

export AWS_PROFILE=AdministratorAccess-770923989980
export AWS_REGION=ap-northeast-1

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

INSTANCE_ID="i-0f0e561633f2e4c03"

echo -e "${RED}ç¾åœ¨ã®å•é¡Œ:${NC}"
echo "â€¢ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å‰Šé™¤ã•ã‚Œã‚‹ã ã‘ã§å‡¦ç†ã•ã‚Œã¦ã„ãªã„"
echo "â€¢ OpenSearchã¸ã®ãƒ‡ãƒ¼ã‚¿ç™»éŒ²ãªã—"
echo "â€¢ DocuWorksãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ãªã—"
echo "â€¢ ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰æ¤œç´¢ä¸å¯èƒ½"
echo ""

# Step 1: æ®µéšçš„å‡¦ç†ç‰ˆã®User Dataä½œæˆ
echo -e "${GREEN}Step 1: æ®µéšçš„å‡¦ç†ç‰ˆã®workerä½œæˆ${NC}"

cat > /tmp/phased_processing_userdata.sh << 'EOF'
#!/bin/bash
set +e
exec > >(tee /var/log/user-data.log)
exec 2>&1

echo "=== Phased Processing Worker Started at $(date) ==="

export AWS_REGION="ap-northeast-1"

# æ—¢å­˜ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚’åœæ­¢
systemctl stop minimal-worker.service 2>/dev/null || true
pkill -f "python.*minimal_worker.py" || true

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
yum install -y python3 python3-pip || true

# å¿…è¦ãªPythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆæ®µéšçš„ã«è¿½åŠ ï¼‰
pip3 install boto3==1.26.137 --no-deps || true
pip3 install botocore==1.29.137 --no-deps || true
pip3 install urllib3==1.26.16 || true
pip3 install certifi || true
pip3 install python-dateutil || true
pip3 install six || true
pip3 install jmespath || true

# OpenSearchç”¨ï¼ˆæœ€å°é™ï¼‰
pip3 install opensearch-py==2.2.0 --no-deps || true
pip3 install requests==2.28.2 || true

# æ®µéšçš„å‡¦ç†workerä½œæˆ
cat <<'WORKER' > /opt/phased_worker.py
#!/usr/bin/env python3
"""
Phased Processing Worker - æ®µéšçš„ã«æ©Ÿèƒ½ã‚’å¾©å…ƒ
Phase 1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã®ç¢ºèªã¨ãƒ­ã‚°è¨˜éŒ²
Phase 2: S3ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ç¢ºèª
Phase 3: OpenSearchã¸ã®æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿ç™»éŒ²
"""

import boto3
import json
import time
import sys
import os
from datetime import datetime

print(f"[{datetime.now()}] Phased Processing Worker Starting...")

# è¨­å®š
QUEUE_URL = "https://sqs.ap-northeast-1.amazonaws.com/770923989980/cis-filesearch-index-queue"
REGION = "ap-northeast-1"
MAX_MESSAGES = 3  # å‡¦ç†é€Ÿåº¦ã‚’è½ã¨ã—ã¦ç¢ºå®Ÿã«å‡¦ç†
OPENSEARCH_ENDPOINT = "https://vpc-cis-filesearch-opensearch-xuupcgptq6a4opklfeh65x3uqe.ap-northeast-1.es.amazonaws.com"
INDEX_NAME = "file-metadata"

# ãƒ•ã‚§ãƒ¼ã‚ºãƒ•ãƒ©ã‚°ï¼ˆæ®µéšçš„ã«æœ‰åŠ¹åŒ–ï¼‰
PHASE1_LOG_MESSAGES = True
PHASE2_ACCESS_S3 = True
PHASE3_INDEX_TO_OPENSEARCH = True

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
sqs = boto3.client('sqs', region_name=REGION)
s3 = boto3.client('s3', region_name=REGION)

# OpenSearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
if PHASE3_INDEX_TO_OPENSEARCH:
    try:
        from opensearchpy import OpenSearch
        opensearch = OpenSearch(
            hosts=[{'host': OPENSEARCH_ENDPOINT.replace('https://', ''), 'port': 443}],
            http_compress=True,
            use_ssl=True,
            verify_certs=True,
            ssl_assert_hostname=False,
            ssl_show_warn=False
        )

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        if not opensearch.indices.exists(index=INDEX_NAME):
            opensearch.indices.create(
                index=INDEX_NAME,
                body={
                    "settings": {
                        "number_of_shards": 1,
                        "number_of_replicas": 1
                    },
                    "mappings": {
                        "properties": {
                            "file_name": {"type": "text"},
                            "file_path": {"type": "keyword"},
                            "file_type": {"type": "keyword"},
                            "file_size": {"type": "long"},
                            "content": {"type": "text"},
                            "timestamp": {"type": "date"},
                            "s3_bucket": {"type": "keyword"},
                            "s3_key": {"type": "keyword"},
                            "docuworks_related": {"type": "object"},
                            "processing_status": {"type": "keyword"}
                        }
                    }
                }
            )
            print(f"[{datetime.now()}] Created OpenSearch index: {INDEX_NAME}")

        OPENSEARCH_AVAILABLE = True
        print(f"[{datetime.now()}] OpenSearch connected successfully")

    except Exception as e:
        print(f"[{datetime.now()}] OpenSearch connection failed: {e}")
        OPENSEARCH_AVAILABLE = False
else:
    OPENSEARCH_AVAILABLE = False

# å‡¦ç†çµ±è¨ˆ
message_count = 0
error_count = 0
indexed_count = 0

def process_s3_event(record):
    """S3ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†"""
    s3_info = record.get('s3', {})
    bucket = s3_info.get('bucket', {}).get('name')
    key = s3_info.get('object', {}).get('key')
    size = s3_info.get('object', {}).get('size', 0)

    if not bucket or not key:
        return None

    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’æ§‹ç¯‰
    file_info = {
        'file_name': os.path.basename(key),
        'file_path': key,
        's3_bucket': bucket,
        's3_key': key,
        'file_size': size,
        'timestamp': datetime.utcnow().isoformat(),
        'processing_status': 'pending'
    }

    # æ‹¡å¼µå­ã‚’åˆ¤å®š
    if '.' in key:
        ext = key.rsplit('.', 1)[-1].lower()
        file_info['file_type'] = ext

        # DocuWorksãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹åˆ¥å‡¦ç†
        if ext in ['xdw', 'xbd']:
            file_info['is_docuworks'] = True
            base_name = key.rsplit('.', 1)[0]
            file_info['docuworks_related'] = {
                'original_file': key,
                'pdf_file': f"{base_name}.pdf",
                'text_file': f"{base_name}.txt"
            }
            print(f"[{datetime.now()}] DocuWorks file detected: {key}")

    # Phase 2: S3ã‚¢ã‚¯ã‚»ã‚¹ç¢ºèª
    if PHASE2_ACCESS_S3:
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            head_response = s3.head_object(Bucket=bucket, Key=key)
            file_info['content_type'] = head_response.get('ContentType', 'unknown')
            file_info['last_modified'] = head_response.get('LastModified', '').isoformat() if 'LastModified' in head_response else ''

            # å°ã•ãªãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯å†…å®¹ã‚’å–å¾—
            if ext in ['txt', 'log'] and size < 10000:  # 10KBæœªæº€
                obj_response = s3.get_object(Bucket=bucket, Key=key)
                content = obj_response['Body'].read().decode('utf-8', errors='ignore')
                file_info['content'] = content[:5000]  # æœ€åˆã®5000æ–‡å­—

            file_info['processing_status'] = 'accessed'

        except Exception as e:
            print(f"[{datetime.now()}] S3 access error for {key}: {e}")
            file_info['processing_status'] = 'error'
            file_info['error_message'] = str(e)

    return file_info

# ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
print(f"[{datetime.now()}] Starting main processing loop...")

while True:
    try:
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡
        response = sqs.receive_message(
            QueueUrl=QUEUE_URL,
            MaxNumberOfMessages=MAX_MESSAGES,
            WaitTimeSeconds=10,
            VisibilityTimeout=120,  # å‡¦ç†ã«æ™‚é–“ã‚’ã‹ã‘ã‚‹
            MessageAttributeNames=['All'],
            AttributeNames=['All']
        )

        messages = response.get('Messages', [])

        if messages:
            print(f"[{datetime.now()}] Processing {len(messages)} messages")

            for message in messages:
                try:
                    message_id = message.get('MessageId')
                    receipt_handle = message['ReceiptHandle']

                    # Phase 1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã®ãƒ­ã‚°
                    if PHASE1_LOG_MESSAGES:
                        body = json.loads(message.get('Body', '{}'))
                        print(f"[{datetime.now()}] Message {message_id}: {json.dumps(body)[:200]}...")

                        # S3ã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†
                        if 'Records' in body:
                            for record in body['Records']:
                                if 's3' in record:
                                    file_info = process_s3_event(record)

                                    if file_info and PHASE3_INDEX_TO_OPENSEARCH and OPENSEARCH_AVAILABLE:
                                        # Phase 3: OpenSearchã¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                                        try:
                                            doc_id = file_info['s3_key'].replace('/', '_')
                                            opensearch.index(
                                                index=INDEX_NAME,
                                                id=doc_id,
                                                body=file_info
                                            )
                                            indexed_count += 1
                                            print(f"[{datetime.now()}] Indexed to OpenSearch: {file_info['file_name']}")

                                            # DocuWorksé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç™»éŒ²
                                            if file_info.get('is_docuworks'):
                                                related = file_info.get('docuworks_related', {})
                                                print(f"[{datetime.now()}] DocuWorks related files: {related}")

                                        except Exception as e:
                                            print(f"[{datetime.now()}] OpenSearch indexing error: {e}")

                    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤ï¼ˆå‡¦ç†å®Œäº†ï¼‰
                    sqs.delete_message(
                        QueueUrl=QUEUE_URL,
                        ReceiptHandle=receipt_handle
                    )

                    message_count += 1
                    if message_count % 10 == 0:
                        print(f"[{datetime.now()}] Stats: Processed={message_count}, Indexed={indexed_count}, Errors={error_count}")

                except Exception as e:
                    error_count += 1
                    print(f"[{datetime.now()}] Error processing message: {e}")

                    # ã‚¨ãƒ©ãƒ¼ãŒå¤šã„å ´åˆã¯å‡¦ç†é€Ÿåº¦ã‚’è½ã¨ã™
                    if error_count > 10:
                        time.sleep(5)

        else:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒãªã„å ´åˆ
            time.sleep(2)

    except KeyboardInterrupt:
        print(f"[{datetime.now()}] Shutting down...")
        break
    except Exception as e:
        print(f"[{datetime.now()}] Main loop error: {e}")
        error_count += 1
        time.sleep(5)

print(f"[{datetime.now()}] Final stats: Processed={message_count}, Indexed={indexed_count}, Errors={error_count}")
WORKER

chmod +x /opt/phased_worker.py

# ã‚µãƒ¼ãƒ“ã‚¹ä½œæˆ
cat <<'SERVICE' > /etc/systemd/system/phased-worker.service
[Unit]
Description=Phased Processing Worker
After=network.target

[Service]
Type=simple
ExecStart=/usr/bin/python3 -u /opt/phased_worker.py
Restart=on-failure
RestartSec=30
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
SERVICE

systemctl daemon-reload
systemctl enable phased-worker.service
systemctl start phased-worker.service

echo "=== Phased Processing Worker Started at $(date) ==="
systemctl status phased-worker.service --no-pager || true
EOF

echo -e "${GREEN}âœ… æ®µéšçš„å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆå®Œäº†${NC}"
echo ""

# Step 2: Launch Templateæ›´æ–°
echo -e "${GREEN}Step 2: Launch Templateæ›´æ–°${NC}"

if [[ "$OSTYPE" == "darwin"* ]]; then
    USER_DATA_BASE64=$(base64 -i /tmp/phased_processing_userdata.sh)
else
    USER_DATA_BASE64=$(base64 -w0 /tmp/phased_processing_userdata.sh)
fi

NEW_VERSION=$(aws ec2 create-launch-template-version \
    --launch-template-name cis-filesearch-worker-template \
    --source-version '$Latest' \
    --launch-template-data "{\"UserData\":\"${USER_DATA_BASE64}\"}" \
    --query 'LaunchTemplateVersion.VersionNumber' \
    --output text)

echo "æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä½œæˆ: v$NEW_VERSION"

aws ec2 modify-launch-template \
    --launch-template-name cis-filesearch-worker-template \
    --default-version $NEW_VERSION > /dev/null

echo -e "${GREEN}âœ… Launch Templateæ›´æ–°å®Œäº†${NC}"
echo ""

# Step 3: å‡¦ç†å†…å®¹ã®èª¬æ˜
echo -e "${YELLOW}æ®µéšçš„å‡¦ç†ã®å†…å®¹:${NC}"
echo ""
echo "Phase 1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã®ç¢ºèªã¨ãƒ­ã‚°"
echo "  â€¢ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å†…å®¹ã‚’è©³ç´°ã«ãƒ­ã‚°å‡ºåŠ›"
echo "  â€¢ ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®åˆ¤å®š"
echo "  â€¢ DocuWorksãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º"
echo ""
echo "Phase 2: S3ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹"
echo "  â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—"
echo "  â€¢ å°ã•ã„ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹å–å¾—"
echo ""
echo "Phase 3: OpenSearchã¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"
echo "  â€¢ åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ç™»éŒ²"
echo "  â€¢ DocuWorksé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®ç´ä»˜ã‘æƒ…å ±"
echo "  â€¢ ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰æ¤œç´¢å¯èƒ½ã«"
echo ""

echo -e "${BLUE}æœŸå¾…ã•ã‚Œã‚‹å‡¦ç†é€Ÿåº¦:${NC}"
echo "  æœ€å°æ§‹æˆ: 7109 msg/åˆ†ï¼ˆå‰Šé™¤ã®ã¿ï¼‰"
echo "  Phase 1-3: 100-500 msg/åˆ†ï¼ˆå®Ÿå‡¦ç†ã‚ã‚Šï¼‰"
echo ""

# Step 4: å†èµ·å‹•ç¢ºèª
echo -e "${YELLOW}Step 4: ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å†èµ·å‹•${NC}"
read -p "ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å†èµ·å‹•ã—ã¦å®Ÿå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/n): " -n 1 -r
echo ""

if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å†èµ·å‹•ä¸­..."

    aws autoscaling terminate-instance-in-auto-scaling-group \
        --instance-id $INSTANCE_ID \
        --no-should-decrement-desired-capacity > /dev/null

    echo "çµ‚äº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡..."

    # æ–°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¾…æ©Ÿ
    for i in {1..10}; do
        sleep 30

        NEW_INSTANCE=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names cis-filesearch-ec2-autoscaling \
            --query 'AutoScalingGroups[0].Instances[?LifecycleState==`InService`].InstanceId' \
            --output text)

        if [ -n "$NEW_INSTANCE" ] && [ "$NEW_INSTANCE" != "$INSTANCE_ID" ]; then
            echo -e "${GREEN}âœ… æ–°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èµ·å‹•: $NEW_INSTANCE${NC}"
            break
        fi

        echo "å¾…æ©Ÿä¸­... ($i/10)"
    done

    # å‡¦ç†é–‹å§‹å¾…æ©Ÿ
    echo "å‡¦ç†é–‹å§‹å¾…æ©Ÿä¸­ï¼ˆ90ç§’ï¼‰..."
    sleep 90

    # å‡¦ç†é€Ÿåº¦æ¸¬å®šï¼ˆå®Ÿå‡¦ç†ãªã®ã§é…ããªã‚‹ã¯ãšï¼‰
    echo ""
    echo "å‡¦ç†é€Ÿåº¦æ¸¬å®šä¸­ï¼ˆ60ç§’ï¼‰..."
    START=$(aws sqs get-queue-attributes \
        --queue-url https://sqs.ap-northeast-1.amazonaws.com/770923989980/cis-filesearch-index-queue \
        --attribute-names ApproximateNumberOfMessages \
        --query 'Attributes.ApproximateNumberOfMessages' \
        --output text)

    sleep 60

    END=$(aws sqs get-queue-attributes \
        --queue-url https://sqs.ap-northeast-1.amazonaws.com/770923989980/cis-filesearch-index-queue \
        --attribute-names ApproximateNumberOfMessages \
        --query 'Attributes.ApproximateNumberOfMessages' \
        --output text)

    if [ "$START" -gt "$END" ]; then
        PROCESSED=$((START - END))
        RATE=$((PROCESSED))
        echo -e "${GREEN}âœ… å‡¦ç†é€Ÿåº¦: $RATE msg/åˆ†${NC}"

        if [ "$RATE" -lt 1000 ]; then
            echo -e "${GREEN}ğŸ‘ å®Ÿéš›ã®å‡¦ç†ãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ï¼ˆé€Ÿåº¦ä½ä¸‹ã¯æ­£å¸¸ï¼‰${NC}"
        else
            echo -e "${YELLOW}âš ï¸ ã¾ã å‡¦ç†ãŒè»½ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™${NC}"
        fi
    fi

    echo ""
    echo "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:"
    echo "1. CloudWatch Logsã§å‡¦ç†å†…å®¹ã‚’ç¢ºèª"
    echo "2. OpenSearchã§ãƒ‡ãƒ¼ã‚¿ãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"
    echo "3. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰æ¤œç´¢ãƒ†ã‚¹ãƒˆ"

else
    echo "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
fi